# 4.1 ì „ì²˜ë¦¬

- what is corpus?

: ìì—°ì–´ì²˜ë¦¬ì— ì‚¬ìš©í•˜ëŠ”, ë‹¨ì–´ë“¤ë¡œ ì´ë£¨ì–´ì§„ ë¬¸ì¥ì´ ì½”í¼ìŠ¤ì´ë‹¤. ì½”í¼ìŠ¤ëŠ” ë‹¨ì¼ ì–¸ì–´ ì½”í¼ìŠ¤ì™€ ì´ì¤‘ ì–¸ì–´ ì½”í¼ìŠ¤, ë‹¤ì¤‘ ì–¸ì–´ ì½”í¼ìŠ¤ë¡œ ë‚˜ëˆŒ ìˆ˜ ìˆìŒ. ë³‘ë ¬ ì½”í¼ìŠ¤ëŠ” ë²ˆì—­ë°ì´í„°ì²˜ëŸ¼ í•œ ë¬¸ì¥ì— ëŒ€í•´ ë‘ ê°œ ì´ìƒì˜ ì–¸ì–´ë¡œ ì“°ì—¬ì§„ ê²ƒì„ ì˜ë¯¸.

e.g) ë‚˜ëŠ” í•™êµì— ê°€ëŠ” ê²ƒì„ ì¢‹ì•„í•œë‹¤.

## 1. ì „ì²˜ë¦¬ ê³¼ì •ì˜ ê°œìš”

===ì „ì²˜ë¦¬ ìˆœì„œ ====

1. ì½”í¼ìŠ¤ ìˆ˜ì§‘
2. ì •ì œ
3. ë¬¸ì¥ ë‹¨ìœ„ ë¶„ì ˆ
4. ë¶„ì ˆ
5. ë³‘ë ¬ ì½”í¼ìŠ¤ ì •ë ¬ (ìƒëµ ê°€ëŠ¥)
6. ì„œë¸Œì›Œë“œ ë¶„ì ˆ

## 

## 2. ì •ì œ ê³¼ì •

### [ì •ê·œí‘œí˜„ì‹]

- or í‘œí˜„ [ ] : [2345cde]  ë˜ëŠ” (2|3|4|5|c|d|e)
- ì—°ì†ëœ ìˆ«ì ë˜ëŠ” ì•ŒíŒŒë²³ í‘œí˜„ [-] : [2-5c-e]
- Not í‘œí˜„ [^]

    [^2-5c-e] â†’ 2ë¶€í„° 5ê¹Œì§€, cë¶€í„° 2ê¹Œì§€ë¥¼ ì œì™¸í•œ í•œ ê¸€ì 

- ê·¸ë£¹ ë§Œë“¤ê¸° ( ) : ê·¸ë£¹ì€ ìˆ˜ì‹ì—ì„œ ( ) ì™€ ë¹„ìŠ·í•œ ì—­í• ì´ë¼ê³  ìƒê°í•˜ë©´ ë¨.. ( a + b ) * 5 ë¡œ ë°˜ë³µ í‘œí˜„ì„ ê°„ë‹¨í•˜ê²Œ í•  ìˆ˜ ìˆëŠ” ê²ƒì²˜ëŸ¼, ì •ê·œí‘œí˜„ì‹ì—ì„œì˜ ê·¸ë£¹ë„ ë¹„ìŠ·í•œ ì—­í•  ìˆ˜í–‰
- ? : ì•ì˜ ìˆ˜ì‹í•˜ëŠ” ë¶€ë¶„ì´ ë‚˜íƒ€ë‚˜ì§€ ì•Šê±°ë‚˜ í•œ ë²ˆë§Œ ë‚˜íƒ€ë‚  ë•Œ

    ê·¸ë£¹ì—ì„œ ?ëŠ” ë‹¤ë¥¸ ì—­í• ì„ ìˆ˜í–‰í•˜ê²Œ ë¨

- + : ì•ì˜ ìˆ˜ì‹í•˜ëŠ” ë¶€ë¶„ì´ í•œ ë²ˆ ì´ìƒ ë‚˜íƒ€ë‚  ë•Œ

    x+

    ![4%201%20%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%8E%E1%85%A5%E1%84%85%E1%85%B5%20dd78093b37ee45779959c3e1a52657e0/Untitled.png](4%201%20%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%8E%E1%85%A5%E1%84%85%E1%85%B5%20dd78093b37ee45779959c3e1a52657e0/Untitled.png)

- * : ì•ì˜ ìˆ˜ì‹í•˜ëŠ” ë¶€ë¶„ì´ ë‚˜íƒ€ë‚˜ì§€ ì•Šê±°ë‚˜ ì—¬ëŸ¬ ë²ˆ ë‚˜íƒ€ë‚  ê²½ìš° (ì´ ë§ì„ ì²˜ìŒ ì½ì—ˆì„ ë• ì´ëŸ° ìƒê°ì´ ë“¤ì—ˆë‹¤. ~~ê·¸ëŸ¼ ì´ëŸ¬ë“  ì €ëŸ¬ë“  ìƒê´€ ì—†ë‹¤ëŠ” ì†Œë¦¬ ì•„ë‹ˆì•¼?~~ ë’¤ì— ì˜ˆì‹œì—ì„œ ë³´ê² ì§€ë§Œ \Sì™€ í•¨ê»˜ ì‚¬ìš©í•˜ë©´ì„œ 'ìˆì„ ìˆ˜ë„ ìˆê³  ì—†ì„ ìˆ˜ë„ ìˆëŠ” ë¬¸ì'ë¥¼ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ì„œ ì‚¬ìš©í•œë‹¤. )
- ë°˜ë³µë˜ëŠ” ë¬¸ì í‘œí˜„
    - x{n} : xê°€ 9ë²ˆ ë°˜ë³µ
    - x{n, } : xê°€ 9ë²ˆ ì´ìƒ ë°˜ë³µ
    - x{n,m} : xê°€ në²ˆì—ì„œ më²ˆ ì‚¬ì´ë¥¼ ë°˜ë³µ
- . ì‚¬ìš© : ì–´ë–¤ ê¸€ìë„ ì§€ì¹­í•  ìˆ˜ ìˆëŠ” í‘œí˜„ (like ë£¨ë¯¸íë¸Œ ì¡°ì»¤ì¹´ë“œ)

![4%201%20%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%8E%E1%85%A5%E1%84%85%E1%85%B5%20dd78093b37ee45779959c3e1a52657e0/Untitled%201.png](4%201%20%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%8E%E1%85%A5%E1%84%85%E1%85%B5%20dd78093b37ee45779959c3e1a52657e0/Untitled%201.png)

ì •ê·œì‹ ì•ˆì—ì„œ .ì„ ì‚¬ìš©í•˜ë ¤ë©´  \. ìœ¼ë¡œ ì§€ì •í•´ì•¼ í•¨. ì•„ë˜ ê´€ë ¨ë¬¸ì„œ ì°¸ì¡°[https://stackoverflow.com/questions/13989640/regular-expression-to-match-a-dot](https://stackoverflow.com/questions/13989640/regular-expression-to-match-a-dot)

- ^ì™€ $ : [ ] ì•ˆì— í¬í•¨ë˜ì§€ ì•Šì€ ^ì™€ $ëŠ” ë¼ì¸ì˜ ì‹œì‘ê³¼ ëì„ ì˜ë¯¸í•¨

    ^x$

    ![4%201%20%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%8E%E1%85%A5%E1%84%85%E1%85%B5%20dd78093b37ee45779959c3e1a52657e0/Untitled%202.png](4%201%20%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%8E%E1%85%A5%E1%84%85%E1%85%B5%20dd78093b37ee45779959c3e1a52657e0/Untitled%202.png)

- ì§€ì •ë¬¸ì ì‚¬ìš©

[ì§€ì •ë¬¸ì ](https://www.notion.so/b0566cd7b6074ad0adb10204f06765f8)

### ì •ê·œì‹ íŒ¨í„´ ì—°ìŠµ ì‚¬ì´íŠ¸

[RegexOne - Learn Regular Expressions - Lesson 1: An Introduction, and the ABCs](https://regexone.com/)

### íŒŒì´ì¬ì—ì„œ ì •ê·œ í‘œí˜„ì‹ ì‚¬ìš©í•˜ê¸°

re ëª¨ë“ˆì„ ì´ìš©í•´ íŒŒì´ì¬ì—ì„œ í™œìš©

```python
import re
regex_pattern = '^a...s$'  #ë¬¸ì¥ì˜ ì²˜ìŒê³¼ ëì´ aì™€ së¡œ ëë‚˜ê³ , ê·¸ ì‚¬ì´ì— 3ê°œì˜ ë¬¸ìë¥¼ í¬í•¨ 
text = 'abyss'

## re ëª¨ë“ˆ í•¨ìˆ˜ë¥¼ í†µí•´ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
re.sub(regex_pattern, 'REPLACED', text)
>> 'REPLACED'
```

re ëª¨ë“ˆì˜ ë‚´ì¥í•¨ìˆ˜ëŠ” ì•„ë˜ ë¬¸ì„œë¥¼ ì°¸ì¡°

[https://www.programiz.com/python-programming/regex](https://www.programiz.com/python-programming/regex)

### ì¹˜í™˜ì ì‚¬ìš©

ê·¸ë£¹( ) ìœ¼ë¡œ ì§€ì •ëœ ë¶€ë¶„ì„ ì—­ìŠ¬ë˜ì‹œ\ ë˜ëŠ” $ + ìˆ«ì í˜•ì‹ìœ¼ë¡œ ë³€ìˆ˜ëª…ì²˜ëŸ¼ ê°€ë¦¬í‚¬ ìˆ˜ ìˆìŒ

```python
x = '''abcdefg
12345
ab12
ab1bc2d
12ab
a1b
1a
hijklmnop
'''

regex_pattern = r'([a-z])[0-9]+([a-z])'
to = r'\1\2'

y = '\n'.join([re.sub(regex_pattern, to, x_i) for x_i in x.split('\n')])
```

ìœ„ ì½”ë“œëŠ” xë¥¼ ì—”í„° ë‹¨ìœ„ë¡œ ë‚˜ëˆˆ ë‹¨ì–´ë“¤ì—ì„œ *ë¬¸ì(a-z)*ì™€ *ë¬¸ì(a-z)* ì‚¬ì´ì˜ ìˆ«ì[0-9]ê°€ ìˆì„ ê²½ìš° ë¬¸ìë§Œ ë‚¨ê¸°ë„ë¡ ë‹¨ì–´ë¥¼ ë°”ê¿”ì£¼ì—ˆë‹¤. 

ë°”ë€ í›„ì˜ ë‹¨ì–´ì¸ r'\1\2'ì—ì„œ \1ì€ ì²«ë²ˆì§¸ ê·¸ë£¹ì¸ ë¬¸ì(a-z)ë¥¼ ì§€ì¹­, \2ëŠ” ë‘ë²ˆì§¸ ê·¸ë£¹ ([a-z])ë¥¼ ì§€ì¹­í•œë‹¤. 

## 3. ë¬¸ì¥ ë‹¨ìœ„ ë¶„ì ˆ ê³¼ì •

ë§ˆì¹¨í‘œë§Œì„ ê¸°ì¤€ìœ¼ë¡œ ë¬¸ì¥ ë‹¨ìœ„ ë¶„ì ˆì„ ìˆ˜í–‰í•˜ë©´ ì˜ì–´ ì•½ìë‚˜ ì†Œìˆ˜ì  ë“± ì—¬ëŸ¬ê°€ì§€ ë¬¸ì œì— ë§ˆì£¼ì¹  ìˆ˜ ìˆìŒ. ë”°ë¼ì„œ ë¶„ì ˆì„ ìœ„í•œ ëª¨ë“ˆì„ ë§Œë“¤ê±°ë‚˜ ìì—°ì–´ì²˜ë¦¬ íˆ´í‚·ì¸ NLTKë¥¼ ì´ìš©

[ë¬¸ì¥ ë‹¨ìœ„ ë¶„ì ˆ ì˜ˆì œ]

ìì—°ì–´ì²˜ë¦¬ëŠ” ì¸ê³µì§€ëŠ¥ì˜ í•œ ì¤„ê¸° ì…ë‹ˆë‹¤. ì‹œí€€ìŠ¤ íˆ¬ ì‹œí€€ìŠ¤ì˜ ë“±ì¥ ì´í›„ë¡œ ë”¥ëŸ¬ë‹ì„ í™œìš©í•œ ìì—°ì–´ì²˜ë¦¬ëŠ” ìƒˆë¡œìš´ ì „ê¸°ë¥¼ ë§ì´í•˜ê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤. ë¬¸ì¥ì„ ë°›ì•„ ë‹¨ìˆœíˆ ìˆ˜ì¹˜ë¡œ ë‚˜íƒ€ë‚´ë˜ ì‹œì ˆì„ ë„˜ì–´, ì›í•˜ëŠ”ëŒ€ë¡œ ë¬¸ì¥ì„ ë§Œë“¤ì–´ë‚¼ ìˆ˜ ìˆê²Œ ëœ ê²ƒì…ë‹ˆë‹¤.

```python
import sys, fileinput, re
from nltk.tokenize import sent_tokenize

if __name__ == "__main__":
    for line in fileinput.FileInput('nlp with pytorch chap4.txt'):
        if line.strip() !="": #line.strip() -> ê³µë°±ì„ ì§€ìš°ê³  ë¬¸ìì—´ì„ ë°˜í™˜
            line = re.sub(r'([a-z])\.([A-Z])',r'\1. \2', line.strip())
            
            sentences = sent_tokenize(line.strip())
            
        for s in sentences:
            if s!= "":
                sys.stdout.write(s + "\n")
```

ìœ„ ì½”ë“œëŠ” [a-z]ì™€ [a-z] ì‚¬ì´ì— . ì´ ìˆìœ¼ë©´ í•´ë‹¹ ë¬¸ìì—´ì„ [a-z]. [a-z]ë¡œ ë°˜í™˜í•œë‹¤. ë‚´ê°€ ì´í•´í•˜ê¸°ë¡œëŠ” ë¬¸ì¥ ë§ë¯¸ì— ë§ˆì¹¨í‘œë¥¼ ì°ê³  ë„ì–´ì“°ê¸°ë¥¼ í•˜ëŠ” ë¬¸ì¥ì´ ìˆê³ , í•˜ì§€ ì•ŠëŠ” ë¬¸ì¥ì´ ìˆì–´ì„œ ì „ë¶€ í•œ ì¹¸ ë„ì›Œì£¼ëŠ” ê±¸ë¡œ ë°”ê¾¸ëŠ” ê²ƒ ê°™ë‹¤. í˜¹ì€ sent_tokenizeê°€ ë§ˆì¹¨í‘œ+ë„ì–´ì“°ê¸°ë§Œ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ì¸ì‹í•œë‹¤ê±°ë‚˜?

ì°¾ì•„ë³´ë‹ˆ sent_tokenizeëŠ” regular expressionì„ ì´ìš©í•˜ëŠ” Treebank tokenizer ë°©ë²•ì„ ì‚¬ìš©í•œë‹¤. ì´ Treebank tokenizerëŠ” í…ìŠ¤íŠ¸ê°€ ì´ë¯¸ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬ë˜ì–´ìˆë‹¤ê³  ê°€ì •í•œë‹¤. 

[ë¬¸ì¥ í•©ì¹˜ê¸° ì˜ˆì œ]

ìì—°ì–´ì²˜ë¦¬ëŠ” ì¸ê³µì§€ëŠ¥ì˜ í•œ ì¤„ê¸° ì…ë‹ˆë‹¤. ì‹œí€€ìŠ¤ íˆ¬ ì‹œí€€ìŠ¤ì˜ ë“±ì¥ ì´í›„ë¡œ \n
ë”¥ëŸ¬ë‹ì„ í™œìš©í•œ ìì—°ì–´ì²˜ë¦¬ëŠ” ìƒˆë¡œìš´ ì „ê¸°ë¥¼ ë§ì´í•˜ê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤. ë¬¸ì¥ì„\n
ë°›ì•„ ë‹¨ìˆœíˆ ìˆ˜ì¹˜ë¡œ ë‚˜íƒ€ë‚´ë˜ ì‹œì ˆì„ ë„˜ì–´, ì›í•˜ëŠ”ëŒ€ë¡œ ë¬¸ì¥ì„ ë§Œë“¤ì–´ë‚¼ ìˆ˜ \n
ìˆê²Œ ëœ ê²ƒì…ë‹ˆë‹¤.

```python
import sys, fileinput
from nltk.tokenize import sent_tokenize

if __name__ == "__main__":
    buf = []

    for line in fileinput.input(data):
        if line.strip() != "":
            buf += [line.strip()]
            sentences = sent_tokenize(" ".join(buf))

            if len(sentences) > 1:
                buf = sentences[1:]

                sys.stdout.write(sentences[0] + '\n')

    sys.stdout.write(" ".join(buf) + "\n")
```

ìœ„ ì½”ë“œëŠ” ë¬¸ì¥ì„ í•˜ë‚˜ë¡œ í•©ì¹œ ë’¤ì—, sent_tokenizeë¥¼ ì´ìš©í•˜ì—¬ ì´ë²ˆì—ëŠ” ì œëŒ€ë¡œ ëœ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬í•´ì£¼ê²Œ ëœë‹¤. 

## 4. ë‹¨ì–´ ë‹¨ìœ„ì˜ ë¶„ì ˆ

[í•œêµ­ì–´ ë¶„ì ˆ]

ì „í˜•ì ì¸ ë¬¸ì¥ì—ì„œëŠ” í”„ë¡œê·¸ë¨ë§ˆë‹¤ ì„±ëŠ¥ì´ ë¹„ìŠ·í•˜ì§€ë§Œ, ì‹ ì¡°ì–´ë‚˜ ê³ ìœ ëª…ì‚¬ë¥¼ ì²˜ë¦¬í•  ë•Œ ì‹ ì¡°ì–´ë§ˆë‹¤ ì²˜ë¦¬ ì •ì±…ì´ ë‹¤ë¥´ë‹¤. ë”°ë¼ì„œ ë³¸ì¸ì´ ê°€ì§„ í…ìŠ¤íŠ¸ì˜ ì„±ì§ˆì„ ì˜ íŒŒì•…í•˜ì—¬ í”„ë¡œê·¸ë¨ì„ ì„ íƒí•´ì•¼ í•¨

- Mecab

    : ê°€ì¥ ì†ë„ê°€ ë¹ ë¦„

    ìœˆë„ìš°ì—ì„œëŠ” ì‚¬ìš©ì´ ë¶ˆê°€ëŠ¥í•¨

    ì„¤ì¹˜ëŠ” ì•„ë˜ ì°¸ì¡°

    [ì„¤ì¹˜í•˜ê¸° - KoNLPy 0.4.3 documentation](https://konlpy-ko.readthedocs.io/ko/v0.4.3/install/#ubuntu)

- KoNLPy

    ë‚´ë¶€ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤ì´ ê°ê¸° ë‹¤ë¥¸ ì–¸ì–´ë¡œ ì´ë£¨ì–´ì ¸ í˜¸í™˜ì„± ë¬¸ì œê°€ ë°œìƒí•˜ê±°ë‚˜, ì¼ë¶€ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤ì´ ìë°”ë¡œ êµ¬í˜„ë˜ì–´ Mecabì— ë¹„í•´ ëŒ€ìš©ëŸ‰ ì½”í¼ìŠ¤ ì²˜ë¦¬ì— ë¶ˆë¦¬í•¨ (ì†ë„ ì´ìŠˆ)

    ê·¸ëŸ¬ë‚˜ ì„¤ì¹˜ê°€ ììœ ë¡œì›Œ ë„ë¦¬ ì´ìš©

    ê° KoNLPy í•˜ìœ„ í´ë˜ìŠ¤ì˜ íŠ¹ì§•ì€ ì•„ë˜ë¥¼ ì°¸ì¡°

    [tag Package - KoNLPy 0.5.2 documentation](https://konlpy.org/ko/latest/api/konlpy.tag/#module-konlpy.tag._hannanum)


#5. ìœ ì‚¬ì„±ê³¼ ëª¨í˜¸ì„±
    
   
ìì—°ì–´ì˜ ì¤‘ì˜ì„± ë¬¸ì œë¥¼ ì–´ë–»ê²Œ í•´ê²°í•  ê²ƒì¸ê°€? 

'ì°¨'ëŠ” car, tea, difference, timeì˜ ì˜ë¯¸ë¥¼ ê°€ì§€ê³  ìˆë‹¤. ì´ ì„œë¡œ ë‹¤ë¥¸ ëœ»ì„ 'ì—°ê´€ì˜ë¯¸'ë¼ê³  ë¶€ë¥¸ë‹¤. ê°™ì€ ì—°ê´€ì˜ë¯¸ ì•ˆì—ì„œ ë‹¤ì‹œ ì„¸ë¶€ì˜ë¯¸ê°€ ë‚˜ë‰œë‹¤. ì˜ˆë¥¼ ë“¤ì–´, 'ì°¨ì´'ì™€ ë™ì¼í•œ ì—°ê´€ì˜ë¯¸ë¡œ ì‚¬ìš©í•  ë•Œ, ì´ ì°¨ì´ëŠ” ìˆ˜ì¤€ì„ ë‚˜íƒ€ë‚´ëŠ” ë§ì¼ ìˆ˜ë„ ìˆê³ , ë‘ ìˆ«ìì˜ ëº„ì…ˆì„ ë‚˜íƒ€ë‚¸ ê²ƒì¼ ìˆ˜ë„ ìˆë‹¤. ì´ë ‡ê²Œ ì—°ê´€ì˜ë¯¸ ë‚´ë¶€ì—ì„œ ë‹¬ë¼ì§€ëŠ” ì˜ë¯¸ë¥¼ 'ì„¸ë¶€ì˜ë¯¸'ë¼ê³  ë¶€ë¥¸ë‹¤. 

ìì—°ì–´ ê¸°ë°˜ì˜ ì¸ê³µì§€ëŠ¥ì€ ë™ì¼í•œ ë‹¨ì–´ë¥¼ ì„¸ë¶€ ì˜ë¯¸ë¡œ ë³€í™˜í•˜ëŠ” ê³¼ì •ì´ í•„ìš”í•˜ë‹¤.

## ì‹œì†ŒëŸ¬ìŠ¤ë¥¼ í™œìš©í•œ ë‹¨ì–´ ì˜ë¯¸ íŒŒì•…

ë‹¨ì–´ì˜ ê³„ì¸µêµ¬ì¡°ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ë¡œ êµ¬ì¶•í•œ ê²ƒì„ thesaurus(ì–´íœ˜ë¶„ë¥˜ì‚¬ì „)ì´ë¼ê³  ë¶€ë¦„

### ì›Œë“œë„·

[http://wordnetweb.princeton.edu/perl/webwn](http://wordnetweb.princeton.edu/perl/webwn)

ë‹¨ì–´ì˜ ìƒìœ„ì–´ì™€ í•˜ìœ„ì–´ë¥¼ ì •ë¦¬í•˜ì—¬ ë‹¨ì–´ì˜ ì¤‘ì˜ì„± í•´ì†Œ

í˜„ì¬ëŠ” ì›Œë“œ ì„ë² ë”©ì„ ì‚¬ìš©í•˜ì—¬ ë‹¨ì–´ì²´ê³„(ì˜¨í†¨ë¡œì§€)ë¥¼ ì–´ëŠì •ë„ ì •ë¦½í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì— í˜„ì¬ ë§ì´ ì‚¬ìš©í•˜ì§€ëŠ” ì•ŠìŒ

[í•œêµ­ì–´ ì›Œë“œë„·]

http://wordnet.kaist.ac.kr/

http://korlex.pusan.ac.kr/

### TF-IDF íŠ¹ì§• ì¶”ì¶œ

TF-IDFëŠ” ë¬¸ì„œì—ì„œ íŠ¹ì • ë‹¨ì–´ê°€ ê°€ì§„ ì¤‘ìš”ë„ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ìˆ˜ì¹˜ì´ë‹¤.

$$TF-IDF(w,d)= \frac {TF(w,d)}{IDF(w)}$$

- TF(term frequency) : ë‹¨ì–´ì˜ ë¬¸ì„œ ë‚´ì— ì¶œí˜„í•œ íšŸìˆ˜
- IDF(inverse document frequency) : ë‹¨ì–´ê°€ ì¶œí˜„í•œ ë¬¸ì„œì˜ ìˆ«ì ì—­ìˆ˜

ë‹¨ì–´ì˜ ì¶œí˜„íšŸìˆ˜ì— $\frac {1}{IDF(w)}$ì„ ê³±í•´ì¤Œìœ¼ë¡œì¨ 'the'ì™€ ê°™ì€ ë‹¨ì–´ì˜ ì¤‘ìš”ë„ê°€ ì»¤ì§€ëŠ” ê²ƒì„ ë§‰ëŠ”ë‹¤. 

ì—­ë¬¸ì„œë¹ˆë„ì¸ IDFì‹ì€ ë³´í†µ ì•„ë˜ì™€ ê°™ì€ ì‹ìœ¼ë¡œ êµ¬í•˜ê²Œ ëœë‹¤. 

$${\mathrm {idf}}(t,D)=\log {\frac {|D|}{|\{d\in D:t\in d\}|}}$$

DëŠ” ì „ì²´ ë¬¸ì„œì˜ ê°œìˆ˜, ë¶„ëª¨ëŠ” ê°œë³„ ë¬¸ì„œ d ì¤‘ ë‹¨ì–´ të¥¼ í¬í•¨í•˜ê³  ìˆëŠ” ë¬¸ì„œì˜ ìˆ˜ê°€ ëœë‹¤.

### TF-IDF ê³„ì‚° ì½”ë“œ(1) - TFê³„ì‚°

```python
import pandas as pd

## ë¬¸ì„œ ë‚´ ë‹¨ì–´ì˜ ì¶œí˜„ë¹ˆë„ ì„¸ê¸°
def get_term_frequency(document, word_dict = None):
    if word_dict is None:
        word_dict = {}
    words = document.split()
    '''
    str.split(sep=None, maxsplit=-1)
    https://docs.python.org/ko/3/library/stdtypes.html?highlight=split#str.split
    ë¬¸ìì—´ì„ êµ¬ë¶„ì ê¸°ì¤€ìœ¼ë¡œ ë‚˜ëˆ„ì–´ì„œ maxsplit+1ê°œì˜ ìš”ì†Œë¥¼ ê°€ì§„ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜í•¨
    '''
    
    for w in words:
        word_dict[w] = 1 + (0 if word_dict.get(w) is None else word_dict[w])
    '''
    words ë¦¬ìŠ¤íŠ¸ ìš”ì†Œë“¤ì„ keyë¡œ ì¶”ê°€í•˜ë©´ì„œ ì…ˆ
    íŠ¹ì • ë‹¨ì–´ ìµœì´ˆ ë“±ì¥ì‹œ key ê°’ì€ 1 1 + (0 if word_dict.get(w) is None)
    ë‘ë²ˆ ì´ìƒ ë“±ì¥ ì‹œ key ê°’ì€ 1 + word_dict[w](ì´ì „ê¹Œì§€ ë“±ì¥í•œ ì´í•©)
    '''
        
    return pd.Series(word_dict).sort_values(ascending = False)
		'''
    index:ë‹¨ì–´, value:ë“±ì¥ íšŸìˆ˜ ì¸ Seriesë¡œ return
    '''
```

ë¯¸ë¦¬ ë§Œë“¤ì–´ë†“ì€ ë¬¸ìì—´ doc1ì— ìœ„ í•¨ìˆ˜ë¥¼ ì‹¤í–‰í•˜ë©´, ë‹¤ìŒê³¼ ê°™ì´ indexëŠ” êµ¬ë¶„ìë¡œ ë‚˜ëˆ„ì–´ì§„ ë¬¸ìì—´ì´ê³ , valueëŠ” í•´ë‹¹ ë¬¸ìì˜ ë¬¸ì„œ ë‚´ ë¹ˆë„ê°€ ë˜ëŠ” seriesê°€ ìƒê¸´ë‹¤. 

![https://s3-us-west-2.amazonaws.com/secure.notion-static.com/a4b3e8da-469d-4466-9352-258a37678968/Untitled.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/a4b3e8da-469d-4466-9352-258a37678968/Untitled.png)

..... ë‚œ ì—¬ê¸°ì„œ ì´í•´ê°€ ì•ˆê°€ê³  ì‹œê°„ì„ ë§ì´ ëºì–´ë¨¹ì€ ê²Œ, ëŒ€ì²´ ì™œ word_dictë¥¼ ê·¸ëƒ¥ dictionary í˜•íƒœë¡œ returnì„ ì•ˆí•˜ê³  êµ³ì´ series í˜•íƒœë¡œ ë¦¬í„´í•œê±°ì§€? dictionaryë„ valueê°’ì„ ê¸°ì¤€ìœ¼ë¡œ sort ê°€ëŠ¥í•œ ê²ƒ ê°™ì€ë°? ([https://careerkarma.com/blog/python-sort-a-dictionary-by-value/](https://careerkarma.com/blog/python-sort-a-dictionary-by-value/))

ê·¸ëŸ¬ë©´ sortí•œ ë‹¤ìŒì— ê°€ì¥ í° ê°’ë¶€í„° iterableë¡œ ë§Œë“¤ì–´ì„œ printí•˜ëŠ” ê±´ ì–´ë–¨ê¹Œ? ê·¸ê²Œ ë” ì‹œê°„ì´ ì ê²Œ ê±¸ë¦¬ì§€ ì•Šì„ê¹Œ?

â†’ ì•„ Seriesë¡œ value_countsë¥¼ í•˜ëŠ” ê²Œ dictionaryë¡œ ê³„ì‚°í•˜ëŠ” ê±°ë³´ë‹¤ ë¹¨ë¼ì„œ ê·¸ë ‡êµ¬ë‚˜..

## seriesì™€ dictionaryì˜ ì°¨ì´ (seriesë¥¼ ì‚¬ìš©í•˜ëŠ” ì´ìœ )

[ë‚´ê°€ ê¶ê¸ˆí–ˆë˜ ë‚´ìš©] 

ğŸ¤” seriesëŠ” dictionaryì²˜ëŸ¼ ì“¸ ìˆ˜ë„ ìˆê³ , listì²˜ëŸ¼ë„ ì“¸ ìˆ˜ ìˆê³ , ë°ì´í„°í”„ë ˆì„ì˜ í•œ ì—´ë¡œë„ ìƒê°í•  ìˆ˜ ìˆëŠ” ë“¯ í•˜ë‹¤. ê·¸ëŸ¼ seriesëŠ” ëŒ€ì²´ ì™œ ìˆëŠ”ê±´ê°€.. ë¼ëŠ” ìƒê°ì´ ë“œëŠ”ë°. .. 

ğŸ¤”  listì™€ dictionaryì™€ êµ¬ë¶„ë˜ëŠ” seriesì˜ íŠ¹ì§•ì€ seriesëŠ” ë™ì¼í•œ typeì˜ ë°ì´í„°ë¡œë§Œ êµ¬ì„±ë˜ì–´ì•¼ í•œë‹¤ëŠ” ê²ƒì´ë‹¤. dataframeì€ seriesì˜ ì§‘í•©ì´ë‹¤. â†’  ê·¸ëŸ°ë° ì‹¤ì œë¡œëŠ” indexë“  valueë“  ì„ì–´ ì‚¬ìš© ê°€ëŠ¥...

[í•´ê²°]

- pandas ì•ˆì— series ë¥¼ ì´ìš©í•´ì„œ ê³„ì‚°í•˜ëŠ” ë°©ë²•ì´ dictionaryë³´ë‹¤ í›¨ì”¬ ë¹ ë¥´ë‹¤ê³  í•œë‹¤.
- seriesëŠ” ê³„ì‚°í•  ë•Œ vectorizeí•´ì„œ ê³„ì‚°í•˜ê¸° ë•Œë¬¸ì— dictionaryê°€ ê³„ì‚° ì²˜ë¦¬í•˜ëŠ” ë°©ë²•ë³´ë‹¤ ë¹ ë¦„ (ëŒ€ì‹  ë©”ëª¨ë¦¬ë¥¼ ë” ë§ì´ ë¨¹ëŠ”ë‹¤ê³  í•¨)
- Seriesì˜ íŠ¹ì§•ì€ í•˜ë‚˜ì˜ data typeë§Œ ë“¤ì–´ê°ˆ ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì¸ë°, seriesì˜ indexë“  valueë“  ëª¨ë‘ str, intê°€ ë™ì‹œì— ê°’ìœ¼ë¡œ ë“¤ì–´ê°ˆ ìˆ˜ ìˆë‹¤. Seriesì˜ typeì€ row í•˜ë‚˜ë‹¹ ì •í•´ì§€ëŠ” ê²ƒì´ ì•„ë‹ˆê³  column ì „ì²´ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì •í•´ì§„ë‹¤. ë§Œì•½ strê³¼ intí˜•ì´ ë™ì‹œì— ë“¤ì–´ê°€ ìˆë‹¤ë©´ columnì˜ series typeì€ objectê°€ ëœë‹¤ (ì—¬ëŸ¬ ê°’ì´ ì„ì—¬ìˆìœ¼ë©´ ê·¸ëƒ¥ objectë¡œ ë¬¶ì–´ì„œ ìƒê°í•´ë²„ë¦¼)

### TF-IDF ê³„ì‚° (2) - IDF ê³„ì‚°

```python
## ê° ë‹¨ì–´ì˜ ëª‡ ê°œì˜ ë¬¸ì„œì—ì„œ ë‚˜íƒ€ë‚¬ëŠ”ì§€ (IDF) ì„¸ê¸°
def get_document_frequency(documents):
    dicts = []
    vocab = set([])
    df = {}
    
    for d in documents:
        tf = get_term_frequency(d) # tf = document dì˜ ë‹¨ì–´ë³„ ë“±ì¥íšŸìˆ˜ series
        dicts += [tf] # listì— ê° documentì˜ ë‹¨ì–´ ë¹ˆë„ìˆ˜ Seriesë¥¼ ìš”ì†Œë¡œ ì¶”ê°€
        vocab = vocab | set(tf.keys())  ## set | set ì€ í•©ì§‘í•©ì„ return
	'''
	tf.keys() -> keysëŠ” Seriesì˜ indexë¥¼ returní•˜ëŠ” ë©”ì„œë“œì„ (ë©”ì„œë“œëŠ” classì•ˆì—ì„œ ì •ì˜ëœ í•¨ìˆ˜)
	set(tf.keys())ëŠ” ì§‘í•©ì´ ë˜ê³ , set|set ì€ í•©ì§‘í•©ì„ returní•¨
	forë¬¸ì„ ë§ˆì¹œ vocabì€ ëª¨ë“  documentì— ìˆëŠ” ë‹¨ì–´ì˜ ì§‘í•©ì´ ë¨.
	'''
         
    for v in list(vocab):
        df[v] = 0  # df[v]ì— ë‹¨ì–´ vê°€ ëª‡ê°œì˜ ë¬¸ì„œì—ì„œ ë‚˜íƒ€ë‚¬ëŠ”ì§€ ì €ì¥
        for dict_d in dicts: 
            if dict_d.get(v) is not None:  # get ë©”ì„œë“œëŠ” dictionaryì²˜ëŸ¼ keyì˜ valueë¥¼ ë°˜í™˜
                df[v] += 1  # ië²ˆì§¸ ë¬¸ì„œì¸ dict_dì— ë‹¨ì–´ vê°€ ìˆë‹¤ë©´ IDFì— +1 ì²˜ë¦¬
                
    return pd.Series(df).sort_values(ascending=False)
```

### TF-IDFì˜ ìµœì¢… ê³„ì‚°

```python
## TF-IDFì˜ ìµœì¢… í•¨ìˆ˜

def get_tfidf(docs): ## ê° ë¬¸ì„œë¥¼ listë¡œ ë°›ìŒ
    vocab = {}
    tfs = []
    for d in docs:
        vocab = get_term_frequency(d, vocab)   # vocab = ë‹¨ì–´ ë¹ˆë„ Series (TF)
        tfs += [get_term_frequency(d)]  # ë¬¸ì„œ ìˆœì„œëŒ€ë¡œ TF Seriesë¥¼ listë¡œ ì €ì¥
    df = get_document_frequency(docs)  # ë‹¨ì–´ì˜ IDF

    from operator import itemgetter
    '''
    f = itemgetter(2), the call f(r) returns r[2].
    g = itemgetter(2, 5, 3), the call g(r) returns (r[2], r[5], r[3]).
    '''
    import numpy as np

    stats = []
    for word, freq in vocab.items():
        tfidfs = []
        for idx in range(len(docs)):
            if tfs[idx].get(word) is not None:  ## ië²ˆì§¸ ë¬¸ì„œ ë‹¨ì–´ì—´(tfs[idx])ì—ì„œ íŠ¹ì • ë‹¨ì–´ì˜ ë¹ˆë„(TF)ê°€ 0ì´ ì•„ë‹ˆë©´
                tfidfs += [tfs[idx][word] * np.log(len(docs) / df[word])] ## TF * ì—­ë¬¸ì„œë¹ˆë„ log(ë¬¸ì„œê°œìˆ˜/ë‹¨ì–´ê°€ ë¬¸ì„œì— ë‚˜íƒ€ë‚œ ë¹ˆë„)
            else:
                tfidfs += [0] ## ë§Œì•½ ë¹ˆë„ê°€ 0ì´ë©´ TF-IDFëŠ” ë¶„ìê°€ 0ì´ë¯€ë¡œ ê³„ì‚° í•„ìš”ì„± X

        stats.append((word, freq, *tfidfs, max(tfidfs)))

    return pd.DataFrame(stats, columns=('word',
                                        'frequency',
                                        'doc1',
                                        'doc2',
                                        'doc3',
                                        'max')).sort_values('max', ascending=False)

get_tfidf([doc1, doc2, doc3])
```

ì•„ë˜ì™€ ê°™ì€ ê²°ê³¼ê°’ì„ ì–»ì„ ìˆ˜ ìˆìŒ

![https://s3-us-west-2.amazonaws.com/secure.notion-static.com/9985acfd-f437-4553-9582-ba9d1de34dec/Untitled.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/9985acfd-f437-4553-9582-ba9d1de34dec/Untitled.png)

## íŠ¹ì§• ë²¡í„° ë§Œë“¤ê¸°

### TF í–‰ë ¬ ë§Œë“¤ê¸° (ë‹¨ì–´ ë¹ˆë„ìˆ˜ í–‰ë ¬)

- ë¬¸ì„œë³„ ë‹¨ì–´ì˜ ì¶œí˜„ íšŸìˆ˜ì¸ TFë¥¼ ì°¨ì›ë³„ë¡œ êµ¬ì„±í•´ ë¬¸ì„œ ê°„ ê´€ê³„ë¥¼ ë‚˜íƒ€ë‚´ëŠ” íŠ¹ì§• ë²¡í„°ë¡œ í™œìš©

```python
def get_tf(docs):
    vocab = {} 
    tfs = []
    for d in docs:
        vocab = get_term_frequency(d, vocab)
        tfs +=  [get_term_frequency(d)] ## doc ê°œìˆ˜ë§Œí¼ tf seriesë¥¼ ê°€ì§„ ë¦¬ìŠ¤íŠ¸
        
    from operator import itemgetter
    import numpy as np
    
    stats = []
    for word, freq in vocab.items():
        tf_v = []
        for idx in range(len(docs)):
            if tfs[idx].get(word) is not None:
                tf_v += [tfs[idx][word]]
            else:
                tf_v += [0]
        stats.append((word, freq, *tf_v))

return pd.DataFrame(stats, columns = ('word', 
                                          'frequency', 
                                          'doc1', 
                                          'doc2', 
                                          'doc3')).sort_values('frequency', ascending = False)
```

![https://s3-us-west-2.amazonaws.com/secure.notion-static.com/82de5a69-5901-4008-ad5f-c776ebce5f75/Untitled.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/82de5a69-5901-4008-ad5f-c776ebce5f75/Untitled.png)

- doc1, doc2, doc3ì— í•´ë‹¹í•˜ëŠ” TF(w,doc1), TF(w,doc2) ... ì˜ 3ê°œ ë²¡í„°ëŠ” ë‹¨ì–´ì˜ ì¶œí˜„íšŸìˆ˜ë¥¼ ì •ì˜í•œ íŠ¹ì§• ë²¡í„°ê°€ ëœë‹¤.
- ê·¸ëŸ¬ë‚˜ ì´ëŸ¬í•œ íŠ¹ì§• ë²¡í„°ëŠ” ë¬¸ì„œê°€ ì»¤ì§€ë©´ ì»¤ì§ˆìˆ˜ë¡ ì°¨ì›ì´ ë„ˆë¬´ ì»¤ì§€ê³ , í¬ì†Œí•œ í–‰ë ¬(ëŒ€ë¶€ë¶„ì´ 0ìœ¼ë¡œ ì±„ì›Œì§)ì´ ëœë‹¤ëŠ” ë¬¸ì œì ì´ ìˆë‹¤.

### Context Windowë¥¼ ì‚¬ìš©í•œ ë‹¨ì–´ ì •ë³´ í™œìš©

- TFë¥¼ íŠ¹ì§•ë²¡í„°ë¡œ ì‚¬ìš©í•˜ëŠ” ê²½ìš° ë‹¨ìˆœíˆ ë‹¨ì–´ì˜ ë¹ˆë„ìˆ˜ë§Œì„ ë‚˜ì—´í–ˆê¸° ë•Œë¬¸ì— ë¬¸ì¥ì˜ ë§¥ë½ ë“± ë¶€ê°€ì ì¸ ì •ë³´ê°€ ìœ ì‹¤ëœë‹¤. ì´ëŸ¬í•œ ë¬¸ì œë¥¼ ë§‰ê¸° ìœ„í•´ "ë™ì‹œë°œìƒ"í•˜ëŠ” ë‹¨ì–´ë“¤ì„
